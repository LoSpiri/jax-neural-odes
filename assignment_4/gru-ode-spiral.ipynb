{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc6382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train trajectories: (1000, 100, 2)\n",
      "Converting to polar coordinates...\n",
      "Feature shape: (1000, 100, 2)\n",
      "Alpha - mean: -0.1100, std: 2.9964\n",
      "\n",
      "Training: 800, Validation: 200\n",
      "Model parameters: 4,481\n",
      "Starting training...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (32,) and (1,).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 355\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(x.size\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mx\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mjax.tree_util.tree_leaves(model)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39meqx.is_array(x))\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    354\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m model, train_losses, val_losses = \u001b[43mtrain_model_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m    361\u001b[39m test_predictions = evaluate_simple(model, features_test, alpha_mean, alpha_std)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 170\u001b[39m, in \u001b[36mtrain_model_simple\u001b[39m\u001b[34m(model, features_train, alpha_train, features_val, alpha_val, num_epochs, batch_size, key)\u001b[39m\n\u001b[32m    167\u001b[39m batch_features = features_train[i:batch_end]\n\u001b[32m    168\u001b[39m batch_alpha = alpha_train[i:batch_end]\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m model, opt_state, batch_loss = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m epoch_loss += batch_loss\n\u001b[32m    175\u001b[39m num_batches += \u001b[32m1\u001b[39m\n",
      "    \u001b[31m[... skipping hidden 18 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 138\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(model, opt_state, trajectories, alpha_targets, optimizer)\u001b[39m\n\u001b[32m    135\u001b[39m     preds = jax.vmap(model)(trajectories)\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp.mean((preds - alpha_targets) ** \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m loss, grads = \u001b[43meqx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter_value_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrajectories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m updates, opt_state = optimizer.update(grads, opt_state, model)\n\u001b[32m    140\u001b[39m model = eqx.apply_updates(model, updates)\n",
      "    \u001b[31m[... skipping hidden 12 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 135\u001b[39m, in \u001b[36mtrain_step.<locals>.batch_loss\u001b[39m\u001b[34m(model, trajectories, alpha_targets)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch_loss\u001b[39m(model, trajectories, alpha_targets):\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     preds = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrajectories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp.mean((preds - alpha_targets) ** \u001b[32m2\u001b[39m)\n",
      "    \u001b[31m[... skipping hidden 7 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mSimpleGRUODE.__call__\u001b[39m\u001b[34m(self, trajectory)\u001b[39m\n\u001b[32m    114\u001b[39m     x_obs = trajectory[obs_idx]\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# GRU update\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgru_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Simple output\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m jnp.squeeze(\u001b[38;5;28mself\u001b[39m.output_layer(h))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.7/lib/python3.11/contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args, **kwds):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/jax/jax-neural-odes/.venv/lib/python3.11/site-packages/equinox/nn/_rnn.py:105\u001b[39m, in \u001b[36mGRUCell.__call__\u001b[39m\u001b[34m(self, input, hidden, key)\u001b[39m\n\u001b[32m    103\u001b[39m     bias_n = \u001b[32m0\u001b[39m\n\u001b[32m    104\u001b[39m igates = jnp.split(\u001b[38;5;28mself\u001b[39m.weight_ih @ \u001b[38;5;28minput\u001b[39m + bias, \u001b[32m3\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m hgates = jnp.split(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight_hh\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m, \u001b[32m3\u001b[39m)\n\u001b[32m    106\u001b[39m reset = jnn.sigmoid(igates[\u001b[32m0\u001b[39m] + hgates[\u001b[32m0\u001b[39m])\n\u001b[32m    107\u001b[39m inp = jnn.sigmoid(igates[\u001b[32m1\u001b[39m] + hgates[\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/jax/jax-neural-odes/.venv/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:1141\u001b[39m, in \u001b[36m_forward_operator_to_aval.<locals>.op\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m-> \u001b[39m\u001b[32m1141\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/jax/jax-neural-odes/.venv/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:604\u001b[39m, in \u001b[36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    602\u001b[39m args = (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[32m    606\u001b[39m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/jax/jax-neural-odes/.venv/lib/python3.11/site-packages/jax/_src/numpy/tensor_contractions.py:254\u001b[39m, in \u001b[36mmatmul\u001b[39m\u001b[34m(a, b, precision, preferred_element_type, out_sharding)\u001b[39m\n\u001b[32m    252\u001b[39m a = lax.squeeze(a, \u001b[38;5;28mtuple\u001b[39m(a_squeeze))\n\u001b[32m    253\u001b[39m b = lax.squeeze(b, \u001b[38;5;28mtuple\u001b[39m(b_squeeze))\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m out = \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m  \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_is_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m  \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m  \u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m result = lax.transpose(out, perm)\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m lax._convert_element_type(result, preferred_element_type, output_weak_type)\n",
      "    \u001b[31m[... skipping hidden 13 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/python/jax/jax-neural-odes/.venv/lib/python3.11/site-packages/jax/_src/lax/lax.py:5303\u001b[39m, in \u001b[36m_dot_general_shape_rule\u001b[39m\u001b[34m(lhs, rhs, dimension_numbers, precision, preferred_element_type, out_sharding)\u001b[39m\n\u001b[32m   5300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core.definitely_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n\u001b[32m   5301\u001b[39m   msg = (\u001b[33m\"\u001b[39m\u001b[33mdot_general requires contracting dimensions to have the same \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5302\u001b[39m          \u001b[33m\"\u001b[39m\u001b[33mshape, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m5303\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg.format(lhs_contracting_shape, rhs_contracting_shape))\n\u001b[32m   5305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _dot_general_shape_computation(lhs.shape, rhs.shape, dimension_numbers)\n",
      "\u001b[31mTypeError\u001b[39m: dot_general requires contracting dimensions to have the same shape, got (32,) and (1,)."
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import diffrax as dfx\n",
    "import optax\n",
    "import numpy as np\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "DATA_PATH = 'spirals.npz'\n",
    "TRAIN_SAMPLES = 1000\n",
    "TEST_SAMPLES = 1000\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# Model configuration\n",
    "INPUT_DIM = 5        # Enhanced features: [r, theta, log_r, log_theta, r/theta]\n",
    "HIDDEN_DIM = 32      # Increased hidden size\n",
    "OUTPUT_DIM = 1       # Predict alpha\n",
    "\n",
    "# Training configuration\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 3e-3\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "OUTPUT_FILE = 'alpha_predictions.npy'\n",
    "PLOT_FILE = 'training_results.png'\n",
    "ENABLE_PLOTS = True\n",
    "\n",
    "# ============================================================================\n",
    "# PHYSICS-INFORMED FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def cartesian_to_polar_sequential(xy_trajectory):\n",
    "    \"\"\"\n",
    "    Convert (x, y) trajectory to polar coordinates (r, theta) sequentially.\n",
    "    Avoids data leakage by not using future information.\n",
    "    \n",
    "    Args:\n",
    "        xy_trajectory: (seq_len, 2) array of (x, y) coordinates\n",
    "    Returns:\n",
    "        features: (seq_len, 2) array of [r, theta]\n",
    "    \"\"\"\n",
    "    x = xy_trajectory[:, 0]\n",
    "    y = xy_trajectory[:, 1]\n",
    "    \n",
    "    # Add small epsilon for numerical stability\n",
    "    eps = 1e-6\n",
    "    r = jnp.sqrt(x**2 + y**2 + eps)\n",
    "    theta = jnp.arctan2(y, x)\n",
    "    \n",
    "    # Sequential unwrapping that only uses current/previous values\n",
    "    theta_unwrapped = jnp.zeros_like(theta)\n",
    "    theta_unwrapped = theta_unwrapped.at[0].set(theta[0])\n",
    "    \n",
    "    # Use scan for efficiency\n",
    "    def unwrap_step(carry, i):\n",
    "        prev_theta, theta_vals = carry\n",
    "        current_theta = theta_vals[i]\n",
    "        \n",
    "        # Handle 2π jumps\n",
    "        diff = current_theta - prev_theta\n",
    "        diff = jnp.where(diff > jnp.pi, diff - 2 * jnp.pi, diff)\n",
    "        diff = jnp.where(diff < -jnp.pi, diff + 2 * jnp.pi, diff)\n",
    "        \n",
    "        new_theta = prev_theta + diff\n",
    "        return (new_theta, theta_vals), new_theta\n",
    "    \n",
    "    _, theta_unwrapped = jax.lax.scan(\n",
    "        unwrap_step, \n",
    "        (theta[0], theta), \n",
    "        jnp.arange(1, len(theta))\n",
    "    )\n",
    "    \n",
    "    theta_unwrapped = jnp.concatenate([theta[0:1], theta_unwrapped])\n",
    "    \n",
    "    return jnp.stack([r, theta_unwrapped], axis=1)\n",
    "\n",
    "def enhanced_polar_features(xy_trajectory):\n",
    "    \"\"\"\n",
    "    Enhanced physics-informed features for spiral relationship.\n",
    "    For spiral: r = alpha * theta, so log(r) = log(alpha) + log(theta)\n",
    "    \n",
    "    Args:\n",
    "        xy_trajectory: (seq_len, 2) array of (x, y) coordinates\n",
    "    Returns:\n",
    "        features: (seq_len, 5) array of enhanced features\n",
    "    \"\"\"\n",
    "    basic_features = cartesian_to_polar_sequential(xy_trajectory)\n",
    "    r, theta = basic_features[:, 0], basic_features[:, 1]\n",
    "    \n",
    "    eps = 1e-6\n",
    "    \n",
    "    # Enhanced features that capture the spiral relationship\n",
    "    log_r = jnp.log(r + eps)                    # log(r) = log(α) + log(θ)\n",
    "    log_theta = jnp.log(jnp.abs(theta) + eps)   # log(θ)\n",
    "    r_over_theta = r / (theta + eps)            # Direct estimate of α\n",
    "    \n",
    "    # Stack all features\n",
    "    features = jnp.stack([r, theta, log_r, log_theta, r_over_theta], axis=1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def load_and_preprocess_data(filepath, n_train=None, n_test=None):\n",
    "    \"\"\"Load spiral data and convert to enhanced polar coordinates.\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    data = np.load(filepath)\n",
    "    \n",
    "    xy_train = data['xy_train'][:n_train] if n_train else data['xy_train']\n",
    "    alpha_train = data['alpha_train'][:n_train] if n_train else data['alpha_train']\n",
    "    xy_test = data['xy_test'][:n_test] if n_test else data['xy_test']\n",
    "    \n",
    "    print(f\"Train shape: {xy_train.shape}, Alpha shape: {alpha_train.shape}\")\n",
    "    print(f\"Test shape: {xy_test.shape}\")\n",
    "    \n",
    "    # Convert to enhanced polar coordinates\n",
    "    print(\"Converting to enhanced polar coordinates...\")\n",
    "    train_features = []\n",
    "    for i in range(len(xy_train)):\n",
    "        features = enhanced_polar_features(jnp.array(xy_train[i]))\n",
    "        train_features.append(features)\n",
    "    train_features = jnp.stack(train_features)\n",
    "    \n",
    "    test_features = []\n",
    "    for i in range(len(xy_test)):\n",
    "        features = enhanced_polar_features(jnp.array(xy_test[i]))\n",
    "        test_features.append(features)\n",
    "    test_features = jnp.stack(test_features)\n",
    "    \n",
    "    print(f\"Enhanced polar feature shape: {train_features.shape}\")\n",
    "    print(f\"Features are: [r, theta, log_r, log_theta, r/theta]\")\n",
    "    \n",
    "    # DO NOT normalize polar coordinates to preserve physical relationships\n",
    "    train_features_raw = train_features\n",
    "    test_features_raw = test_features\n",
    "    \n",
    "    # Normalize alpha\n",
    "    alpha_mean = alpha_train.mean()\n",
    "    alpha_std = alpha_train.std()\n",
    "    alpha_train_norm = (alpha_train - alpha_mean) / (alpha_std + 1e-8)\n",
    "    alpha_train_norm = alpha_train_norm.squeeze()\n",
    "    \n",
    "    print(f\"\\nAlpha stats (raw): min={alpha_train.min():.4f}, max={alpha_train.max():.4f}, mean={alpha_mean:.4f}, std={alpha_std:.4f}\")\n",
    "    print(f\"r stats: min={train_features[:,:,0].min():.4f}, max={train_features[:,:,0].max():.4f}\")\n",
    "    print(f\"theta stats: min={train_features[:,:,1].min():.4f}, max={train_features[:,:,1].max():.4f}\")\n",
    "    \n",
    "    return (train_features_raw,\n",
    "            jnp.array(alpha_train_norm),\n",
    "            test_features_raw,\n",
    "            float(alpha_mean),\n",
    "            float(alpha_std),\n",
    "            alpha_train,\n",
    "            xy_train,\n",
    "            xy_test)\n",
    "\n",
    "# ============================================================================\n",
    "# GRU-ODE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "class ODEFunc(eqx.Module):\n",
    "    \"\"\"ODE function for continuous evolution between observations\"\"\"\n",
    "    mlp: eqx.nn.MLP\n",
    "    hidden_size: int\n",
    "\n",
    "    def __init__(self, hidden_size: int, *, key):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_size,\n",
    "            out_size=hidden_size,\n",
    "            width_size=hidden_size * 2,\n",
    "            depth=2,\n",
    "            activation=jax.nn.softplus,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, h, args):\n",
    "        h = jnp.reshape(h, (-1,))\n",
    "        return self.mlp(h)\n",
    "\n",
    "\n",
    "class GRUCell(eqx.Module):\n",
    "    \"\"\"GRU Cell for observation updates\"\"\"\n",
    "    Wz: jnp.ndarray\n",
    "    Wr: jnp.ndarray\n",
    "    Wh: jnp.ndarray\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, key):\n",
    "        key_z, key_r, key_h = random.split(key, 3)\n",
    "        \n",
    "        scale = 1.0 / jnp.sqrt(hidden_size)\n",
    "        self.Wz = random.normal(key_z, (hidden_size + input_size, hidden_size)) * scale\n",
    "        self.Wr = random.normal(key_r, (hidden_size + input_size, hidden_size)) * scale\n",
    "        self.Wh = random.normal(key_h, (hidden_size + input_size, hidden_size)) * scale\n",
    "    \n",
    "    def __call__(self, x, h_prev):\n",
    "        combined = jnp.concatenate([h_prev, x], axis=-1)\n",
    "        z = jax.nn.sigmoid(combined @ self.Wz)\n",
    "        r = jax.nn.sigmoid(combined @ self.Wr)\n",
    "        combined_reset = jnp.concatenate([r * h_prev, x], axis=-1)\n",
    "        h_prime = jnp.tanh(combined_reset @ self.Wh)\n",
    "        h = (1 - z) * h_prime + z * h_prev\n",
    "        return h\n",
    "\n",
    "\n",
    "class PhysicsInformedGRUODE(eqx.Module):\n",
    "    \"\"\"\n",
    "    Enhanced Physics-informed GRU-ODE: Uses spiral physics features\n",
    "    \"\"\"\n",
    "    ode_func: ODEFunc\n",
    "    gru_cell: GRUCell\n",
    "    readout: eqx.nn.MLP\n",
    "    hidden_size: int\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, *, key):\n",
    "        key_ode, key_gru, key_readout = random.split(key, 3)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ode_func = ODEFunc(hidden_size, key=key_ode)\n",
    "        self.gru_cell = GRUCell(input_size, hidden_size, key=key_gru)\n",
    "        \n",
    "        # More powerful readout network\n",
    "        self.readout = eqx.nn.MLP(\n",
    "            in_size=hidden_size,\n",
    "            out_size=output_size,\n",
    "            width_size=hidden_size * 2,\n",
    "            depth=3,\n",
    "            activation=jax.nn.softplus,\n",
    "            key=key_readout\n",
    "        )\n",
    "\n",
    "    def __call__(self, trajectory):\n",
    "        \"\"\"\n",
    "        trajectory: (seq_len, 5) - enhanced polar coordinates\n",
    "        The model learns: r = alpha * theta\n",
    "        \"\"\"\n",
    "        seq_len = trajectory.shape[0]\n",
    "        h = jnp.zeros((self.hidden_size,), dtype=jnp.float32)\n",
    "        ts = jnp.linspace(0, 1, seq_len)\n",
    "        \n",
    "        solver = dfx.Dopri5()\n",
    "        term = dfx.ODETerm(self.ode_func)\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            if i > 0:\n",
    "                t0, t1 = ts[i-1], ts[i]\n",
    "                solution = dfx.diffeqsolve(\n",
    "                    term,\n",
    "                    solver,\n",
    "                    t0=t0,\n",
    "                    t1=t1,\n",
    "                    dt0=(t1 - t0) / 10.0,  # More steps for accuracy\n",
    "                    y0=h,\n",
    "                    saveat=dfx.SaveAt(t1=True),\n",
    "                    max_steps=32,  # Increased max steps\n",
    "                    stepsize_controller=dfx.PIDController(rtol=1e-3, atol=1e-6)\n",
    "                )\n",
    "                h = jnp.reshape(solution.ys, (-1,))\n",
    "            \n",
    "            x_obs = trajectory[i]\n",
    "            h = self.gru_cell(x_obs, h)\n",
    "        \n",
    "        alpha_pred = self.readout(h)\n",
    "        return jnp.squeeze(alpha_pred)\n",
    "\n",
    "# ============================================================================\n",
    "# PHYSICS-INFORMED LOSS\n",
    "# ============================================================================\n",
    "\n",
    "def physics_informed_loss(model, trajectory, alpha_target, alpha_std):\n",
    "    \"\"\"Enhanced loss with physics-based regularization\"\"\"\n",
    "    pred = model(trajectory)\n",
    "    alpha_target = jnp.squeeze(alpha_target)\n",
    "    \n",
    "    # Standard MSE\n",
    "    mse_loss = jnp.mean((pred - alpha_target) ** 2)\n",
    "    \n",
    "    # Physics-based regularization: check if r ≈ αθ holds\n",
    "    r, theta = trajectory[:, 0], trajectory[:, 1]\n",
    "    eps = 1e-6\n",
    "    alpha_physical = r / (theta + eps)\n",
    "    \n",
    "    # Use robust statistics (median) to avoid outliers\n",
    "    alpha_physical_median = jnp.median(alpha_physical)\n",
    "    \n",
    "    # Encourage predictions to match physical relationship\n",
    "    physics_loss = jnp.mean((pred - alpha_physical_median) ** 2)\n",
    "    \n",
    "    # Combined loss with smaller physics weight to avoid over-constraining\n",
    "    return mse_loss + 0.05 * physics_loss\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "@eqx.filter_jit\n",
    "def train_step(model, opt_state, trajectory, alpha_target, optimizer, alpha_std):\n",
    "    loss, grads = eqx.filter_value_and_grad(physics_informed_loss)(\n",
    "        model, trajectory, alpha_target, alpha_std\n",
    "    )\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss\n",
    "\n",
    "def compute_validation_loss(model, features_val, alpha_val, alpha_std):\n",
    "    \"\"\"Compute validation loss\"\"\"\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(features_val)):\n",
    "        trajectory = features_val[i]\n",
    "        alpha_target = alpha_val[i]\n",
    "        loss = physics_informed_loss(model, trajectory, alpha_target, alpha_std)\n",
    "        total_loss += loss\n",
    "        count += 1\n",
    "    \n",
    "    return total_loss / count\n",
    "\n",
    "def train_model(model, features_train, alpha_train, features_val, alpha_val, \n",
    "                alpha_std, num_epochs, batch_size, key):\n",
    "    \"\"\"Training loop with early stopping\"\"\"\n",
    "    # Setup optimizer with learning rate scheduling\n",
    "    scheduler = optax.exponential_decay(\n",
    "        init_value=LEARNING_RATE,\n",
    "        transition_steps=len(features_train) // batch_size * 10,\n",
    "        decay_rate=0.95\n",
    "    )\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip(1.0),\n",
    "        optax.adam(scheduler)\n",
    "    )\n",
    "    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "    \n",
    "    num_samples = features_train.shape[0]\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = model\n",
    "    patience = 8\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        key, subkey = random.split(key)\n",
    "        perm = random.permutation(subkey, num_samples)\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_idx = perm[i:i+batch_size]\n",
    "            \n",
    "            batch_loss = 0.0\n",
    "            batch_count = 0\n",
    "            for j in batch_idx:\n",
    "                trajectory = features_train[j]\n",
    "                alpha_target = alpha_train[j]\n",
    "                \n",
    "                model, opt_state, step_loss = train_step(\n",
    "                    model, opt_state, trajectory, alpha_target, optimizer, alpha_std\n",
    "                )\n",
    "                batch_loss += step_loss\n",
    "                batch_count += 1\n",
    "            \n",
    "            batch_loss /= batch_count\n",
    "            epoch_loss += batch_loss\n",
    "            num_batches += 1\n",
    "            \n",
    "            if (num_batches % 10) == 0:\n",
    "                print(f\"  Batch {num_batches}, Loss: {batch_loss:.6f}\")\n",
    "        \n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        train_losses.append(float(avg_train_loss))\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = compute_validation_loss(model, features_val, alpha_val, alpha_std)\n",
    "        val_losses.append(float(val_loss))\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model\n",
    "            patience_counter = 0\n",
    "            print(f\"  New best validation loss: {best_val_loss:.6f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{patience})\")\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "    return best_model, train_losses, val_losses\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "@eqx.filter_jit\n",
    "def predict_single(model, trajectory):\n",
    "    return model(trajectory)\n",
    "\n",
    "def predict_batch(model, trajectories):\n",
    "    predictions = []\n",
    "    for i in range(trajectories.shape[0]):\n",
    "        pred = predict_single(model, trajectories[i])\n",
    "        predictions.append(pred)\n",
    "    return jnp.array(predictions)\n",
    "\n",
    "def evaluate_model(model, features_test, alpha_mean, alpha_std):\n",
    "    print(\"Evaluating on test set...\")\n",
    "    predictions_norm = predict_batch(model, features_test)\n",
    "    predictions = predictions_norm * alpha_std + alpha_mean\n",
    "    return predictions\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data with enhanced physics-informed features\n",
    "    features_train, alpha_train, features_test, alpha_mean, alpha_std, alpha_train_raw, xy_train, xy_test = \\\n",
    "        load_and_preprocess_data(DATA_PATH, TRAIN_SAMPLES, TEST_SAMPLES)\n",
    "    \n",
    "    # Split validation set\n",
    "    n_train = int(features_train.shape[0] * (1 - VALIDATION_SPLIT))\n",
    "    features_val = features_train[n_train:]\n",
    "    alpha_val = alpha_train[n_train:]\n",
    "    alpha_val_raw = alpha_train_raw[n_train:]\n",
    "    xy_val = xy_train[n_train:]\n",
    "    features_train = features_train[:n_train]\n",
    "    alpha_train = alpha_train[:n_train]\n",
    "    alpha_train_raw = alpha_train_raw[:n_train]\n",
    "    \n",
    "    print(f\"\\nTraining samples: {features_train.shape[0]}\")\n",
    "    print(f\"Validation samples: {features_val.shape[0]}\")\n",
    "    print(f\"Input dimension: {INPUT_DIM}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    key = random.PRNGKey(RANDOM_SEED)\n",
    "    key, model_key = random.split(key)\n",
    "    \n",
    "    model = PhysicsInformedGRUODE(\n",
    "        input_size=INPUT_DIM,\n",
    "        hidden_size=HIDDEN_DIM,\n",
    "        output_size=OUTPUT_DIM,\n",
    "        key=model_key\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel: Enhanced Physics-Informed GRU-ODE\")\n",
    "    print(\"Input: Enhanced polar coordinates [r, theta, log_r, log_theta, r/theta]\")\n",
    "    print(\"Learning: r = alpha * theta with physics-informed loss\")\n",
    "    \n",
    "    # Train model\n",
    "    model, train_losses, val_losses = train_model(\n",
    "        model, features_train, alpha_train, features_val, alpha_val,\n",
    "        alpha_std, NUM_EPOCHS, BATCH_SIZE, key\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"\\nEvaluating on validation set...\")\n",
    "    val_predictions_norm = predict_batch(model, features_val)\n",
    "    val_predictions = val_predictions_norm * alpha_std + alpha_mean\n",
    "    \n",
    "    print(f\"Val predictions (denormalized) - min: {val_predictions.min():.4f}, max: {val_predictions.max():.4f}, mean: {val_predictions.mean():.4f}\")\n",
    "    print(f\"Val targets (actual) - min: {alpha_val_raw.min():.4f}, max: {alpha_val_raw.max():.4f}, mean: {alpha_val_raw.mean():.4f}\")\n",
    "    \n",
    "    val_mse = jnp.mean((val_predictions - alpha_val_raw.squeeze()) ** 2)\n",
    "    val_mae = jnp.mean(jnp.abs(val_predictions - alpha_val_raw.squeeze()))\n",
    "    val_rmse = jnp.sqrt(val_mse)\n",
    "    \n",
    "    print(f\"Validation MSE: {val_mse:.6f}\")\n",
    "    print(f\"Validation MAE: {val_mae:.6f}\")\n",
    "    print(f\"Validation RMSE: {val_rmse:.6f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_predictions = evaluate_model(model, features_test, alpha_mean, alpha_std)\n",
    "    \n",
    "    # Save predictions\n",
    "    np.save(OUTPUT_FILE, np.array(test_predictions))\n",
    "    print(f\"\\nPredictions saved to {OUTPUT_FILE}\")\n",
    "    \n",
    "    # Plot results\n",
    "    if ENABLE_PLOTS:\n",
    "        fig = plt.figure(figsize=(18, 14))\n",
    "        gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Training and validation loss\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        ax1.plot(train_losses, label='Train Loss')\n",
    "        ax1.plot(val_losses, label='Val Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss (Normalized)')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Predictions vs actual\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        ax2.scatter(alpha_val_raw, val_predictions, alpha=0.5)\n",
    "        min_val = min(alpha_val_raw.min(), val_predictions.min())\n",
    "        max_val = max(alpha_val_raw.max(), val_predictions.max())\n",
    "        ax2.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect')\n",
    "        ax2.set_xlabel('True Alpha')\n",
    "        ax2.set_ylabel('Predicted Alpha')\n",
    "        ax2.set_title(f'Validation (MSE: {val_mse:.4f}, MAE: {val_mae:.4f})')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        ax2.axis('equal')\n",
    "        \n",
    "        # Residuals\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        residuals = val_predictions - alpha_val_raw.squeeze()\n",
    "        ax3.scatter(alpha_val_raw, residuals, alpha=0.5)\n",
    "        ax3.axhline(y=0, color='r', linestyle='--')\n",
    "        ax3.set_xlabel('True Alpha')\n",
    "        ax3.set_ylabel('Residual')\n",
    "        ax3.set_title('Residuals')\n",
    "        ax3.grid(True)\n",
    "        \n",
    "        # Error distribution\n",
    "        ax4 = fig.add_subplot(gs[1, 0])\n",
    "        ax4.hist(residuals, bins=20, alpha=0.7, edgecolor='black')\n",
    "        ax4.axvline(x=0, color='r', linestyle='--')\n",
    "        ax4.set_xlabel('Prediction Error')\n",
    "        ax4.set_ylabel('Frequency')\n",
    "        ax4.set_title('Error Distribution')\n",
    "        ax4.grid(True)\n",
    "        \n",
    "        # Plot 6 example spirals (2 rows x 3 cols)\n",
    "        for plot_idx in range(6):\n",
    "            row = 2 + plot_idx // 3\n",
    "            col = plot_idx % 3\n",
    "            ax = fig.add_subplot(gs[row, col])\n",
    "            \n",
    "            val_idx = plot_idx * (len(xy_val) // 6)\n",
    "            trajectory_actual = xy_val[val_idx]\n",
    "            \n",
    "            true_alpha = float(alpha_val_raw[val_idx])\n",
    "            pred_alpha = float(val_predictions[val_idx])\n",
    "            \n",
    "            # Plot actual trajectory\n",
    "            ax.plot(trajectory_actual[:, 0], trajectory_actual[:, 1], \n",
    "                   'b-', linewidth=2, label=f'True α={true_alpha:.2f}', alpha=0.7)\n",
    "            ax.plot(trajectory_actual[0, 0], trajectory_actual[0, 1], \n",
    "                   'go', markersize=8, label='Start')\n",
    "            \n",
    "            # Generate predicted spiral\n",
    "            theta_pred = np.linspace(0, 4*np.pi, 100)\n",
    "            r_pred = pred_alpha * theta_pred\n",
    "            x_pred = r_pred * np.cos(theta_pred)\n",
    "            y_pred = r_pred * np.sin(theta_pred)\n",
    "            ax.plot(x_pred, y_pred, 'r--', linewidth=1.5, \n",
    "                   label=f'Pred α={pred_alpha:.2f}', alpha=0.7)\n",
    "            \n",
    "            ax.set_xlabel('X')\n",
    "            ax.set_ylabel('Y')\n",
    "            ax.set_title(f'Example {plot_idx+1} (Error: {pred_alpha-true_alpha:.2f})')\n",
    "            ax.legend(fontsize=8)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.axis('equal')\n",
    "        \n",
    "        plt.savefig(PLOT_FILE, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Plot saved to {PLOT_FILE}\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-neural-odes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
